{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/ML2-Equipo_7-sep-2023/blob/main/A4_DL_TC5033_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OzS5vKtz5EFy",
      "metadata": {
        "id": "OzS5vKtz5EFy"
      },
      "source": [
        "\n",
        "![Evidence 3](https://i.imgur.com/mu6ZuGT.jpg)\n",
        "\n",
        "# **Master's in Applied Artificial Intelligence**\n",
        "## **Course: Advanced Machine Learning Methods**\n",
        "* ### **Lead Instructor**: José Antonio Cantoral Ceballos\n",
        "\n",
        "## **Activity 4: Building a Simple LSTM Text Generator using WikiText-2**\n",
        "\n",
        "* ### **Members - Team 7**\n",
        "\n",
        "*   --> Eduardo Gabriel Arévalo Aguilar - A01793897\n",
        "*   --> David Andrés González Medina - A01794025\n",
        "*   --> Maricel Parra Osorio - A01793932\n",
        "*   --> Yeison Fernando Villamil Franco - A01793803"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iI_boi80zH1m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI_boi80zH1m",
        "outputId": "08bc3b71-02a1-4f9b-a7de-3b0dc3117239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('i like to listen music , my favorite artist is juanes ( complex and ) played '\n",
            " '. the following a upper central @-@ recommendation case , haifa warsaw alice '\n",
            " '. from the key was secured by <unk> and march disease . in march patience '\n",
            " 'toured serves of the long florida perspectives in september 2013 was for the '\n",
            " 'toilet . weaving <unk>')\n"
          ]
        }
      ],
      "source": [
        "# Defining text generation function\n",
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "    model.eval()\n",
        "    words = tokeniser(start_text)\n",
        "    hidden = model.init_hidden(1)\n",
        "    for i in range(0, num_words):\n",
        "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
        "        y_pred, hidden = model(x, hidden)\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Example text generation\n",
        "start_text=\"I like to listen music, my favorite artist is Juanes\"\n",
        "num_words=50\n",
        "temperature=1\n",
        "pprint(generate_text(model, start_text, num_words, temperature))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dHmGPHfxZwA4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1250
        },
        "id": "dHmGPHfxZwA4",
        "outputId": "73c17bba-4759-4a67-fda8-2eacfbf6f39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "Collecting portalocker==2.2\n",
            "  Using cached portalocker-2.2.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "  Attempting uninstall: portalocker\n",
            "    Found existing installation: portalocker 2.8.2\n",
            "    Uninstalling portalocker-2.8.2:\n",
            "      Successfully uninstalled portalocker-2.8.2\n",
            "Successfully installed portalocker-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "portalocker"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Collecting portalocker\n",
            "  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "  Attempting uninstall: portalocker\n",
            "    Found existing installation: portalocker 2.2.0\n",
            "    Uninstalling portalocker-2.2.0:\n",
            "      Successfully uninstalled portalocker-2.2.0\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "portalocker"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# conda install -c pytorch torchtext\n",
        "# conda install -c pytorch torchdata\n",
        "# conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
        "\n",
        "### Google Colab\n",
        "!conda install -c pytorch torchtext==0.8 torchaudio cudatoolkit=10.2 -c pytorch\n",
        "!pip install portalocker==2.2\n",
        "!pip install scikit-plot\n",
        "!pip install --upgrade portalocker\n",
        "\n",
        "### Librería LIME para visualizar mejor las predicciones\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from pprint import pprint\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [],
      "source": [
        "# Checking if CUDA (GPU) is available, otherwise using CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "# Loading WikiText-2 dataset\n",
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the data\n",
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "# Defining a function for data processing\n",
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n",
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "# Creating TensorDatasets for training, validation, and test\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ywtfpstm5lD0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywtfpstm5lD0",
        "outputId": "ee130052-814a-401c-ca67-02e714490328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40999 4288 4837\n",
            "40999 4288 4837\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset), len(val_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "# Setting up data loaders\n",
        "batch_size = 256 # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **The following funcion of applying an neural network `LSTM` consist in three components:**\n",
        "\n",
        "* In the init constructor it's important to specify the principal component of the NN; embeddings, hidder_size, num_layers and the owns components of the LSTM with a output layer. At this point, when a model is instanced, declare the hidden layer to avoid errors.\n",
        "* The forward component specify the three principal elements to be considered; embeddings as traductor text, output layer of the encoder (embedding) and the decoder which is in charge of decoding the text.\n",
        "* init_hidden has the purpose to consider the component layer that do all the process to understand the patterns of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        embeddings = self.embeddings(text)\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        decoded = self.fc(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 1 # the number of nn.LSTM layers\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**As was mentioned above, to avoid error it's important to declare the hidden layer. One of the biggest challenge for this type of exercise is to find a correct metric that allow us measure the coherence of the text. Metrics as a BLEU metric and others that have the capabilities to measure distance similarity, which the approach is to find each token and compared if the response text has the token or word of the original text.. But for this purpose, we only declare the cost or loss function and the others components of the NN: `dataset`, `hidden layer`, `loss` for each `epoch`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "# Defining the training function\n",
        "def train(model, epochs, optimiser):\n",
        "    '''\n",
        "    The following are possible instructions you may want to conside for this function.\n",
        "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
        "    as long as you train your model correctly.\n",
        "        - loop through specified epochs\n",
        "        - loop through dataloader\n",
        "        - don't forget to zero grad!\n",
        "        - place data (both input and target) in device\n",
        "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "        - run the model\n",
        "        - compute the cost or loss\n",
        "        - backpropagation\n",
        "        - Update paratemers\n",
        "        - Include print all the information you consider helpful\n",
        "\n",
        "    '''\n",
        "    model = model.to(device=device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Progress bar for training\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
        "\n",
        "        for i, (xi, yi) in enumerate(progress_bar):\n",
        "            # Move data to device\n",
        "            xi, yi = xi.to(device), yi.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(xi, hidden)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(outputs.view(-1, vocab_size), yi.view(-1))\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            # Update total loss for the epoch\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update progress bar with current loss\n",
        "            progress_bar.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9c84ce",
        "outputId": "0a6cb015-75ac-4161-9a2c-b592ce3ea42b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Average Loss: 6.3357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Average Loss: 6.1975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Average Loss: 6.0822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Average Loss: 5.9913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Average Loss: 5.9161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.0005\n",
        "epochs = 5\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training the model\n",
        "train(model, epochs, optimiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**As showed above by using the `generate_text` function, we taked an example for creating a paragraph with a total of 50 word considering a initial text  like `I like to listen music, my favorite artist is Juanes`. The purpose of the LSMT model is find in your memory how to complete the sentence with the number of words indicated and with other parameters such as `temperature`. This parameter allow us to obtain a exactly response or a randomless response (variying between 0 and 1 respectively). The result can be different for each iteration, but for the example it's possible to observe that the model is able to create new text with the example indicated. Which is a good approximation for the training data and time.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E5G89zmrBLQ1",
      "metadata": {
        "id": "E5G89zmrBLQ1"
      },
      "source": [
        "# **Conclusions**\n",
        "\n",
        "1. LSTM Model for Sequence Processing: An LSTM model has been implemented for sequential data processing using PyTorch. The architecture includes embedding layers, an LSTM layer, and a linear layer for predicting the next token in the sequence.\n",
        "\n",
        "2. Model Training: A training function has been implemented to iterate over the specified epochs and data batches, conducting backpropagation to refine the model parameters. The training process employs the cross-entropy loss function and utilizes the Adam optimizer for effective parameter updates.\n",
        "\n",
        "3. Text Generation: A function has been implemented to generate text from an initial seed using the trained model. The generation involves sampling from the probability distribution of logits for the last token, considering a specified temperature.\n",
        "\n",
        "4. Examples of generated text by the model have been provided. It's important to note that due to the simplicity of the model, the amount of data, and limited computational resources, the results may not make much sense and are primarily used for academic purposes.\n",
        "\n",
        "5. It was observed that by modifying some of the hyperparameters of the model, it was expected to obtain a more accurate or more precise result. However, the execution of the training function for text generation can take random values that are difficult to fit to the meaning of the sentence.\n",
        "\n",
        "6. Because the model uses a library that receives information from millions of references around the world, text generation interpretations can vary significantly, depending on the level of relationship contained in the data used in training the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
